{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Shared_Task.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyNM4EWMhwfIfVEWfIH931UU",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rfahrn/Shared_Task/blob/main/Shared_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install emoji\n",
    "! pip install pyarabic\n",
    "! pip install transformers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! git clone https://github.com/rfahrn/Shared_Task.git"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import unicodedata\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarabic.araby as ar\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-mini-arabic\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/offenseval-ar-training-v1.tsv', sep='\\t')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preview data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jessie/Downloads/UZH/text_mining/Assignments/ex_05/Shared_Task/venv/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Text(0.5, 1.0, 'count NOT/OFF')"
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(train.subtask_a)\n",
    "plt.title('count NOT/OFF')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that there are much more non-offensive than offensive tweets - a common issue in HSD data. Transformers tend to perform better with balanced datasets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Text Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vb/9rxy0nsn1sz0zd7f5_x0rkyr0000gp/T/ipykernel_85766/857312938.py:5: DeprecationWarning: 'emoji.get_emoji_regexp()' is deprecated and will be removed in version 2.0.0. If you want to remove emoji from a string, consider the method emoji.replace_emoji(str, replace='').\n",
      "To hide this warning, pin/downgrade the package to 'emoji~=1.6.3'\n",
      "  return emoji.get_emoji_regexp().sub(u'', tweet)\n"
     ]
    },
    {
     "data": {
      "text/plain": "   id                                              tweet  subtask_a\n0   1  الحمدلله يارب فوز مهم يا زمالك.. كل الدعم ليكم...          0\n1   2            فدوه يا بخت فدوه يا زمن واحد منكم يجيبه          0\n2   3  RT @USER: يا رب يا واحد يا احد بحق يوم الاحد ا...          1\n3   4  RT @USER: #هوا_الحرية يا وجع قلبي عليكي يا امي...          0\n4   5           يا بكون بحياتك الاهم يا اما ما بدي اكون           0\n5   6  @USER اخخ يا قلببي يا هالحلقه  متعه على بكاء ع...          0\n6   7  والله الزول السوداني اثبت انه سابق بالتحضر عن ...          0\n7   8  RT @USER: جالس اسمع احمد قاسم يغني: \"احبك من ك...          0\n8   9                في قلبي يا مغلاك وبعيني يا محلاك ..          0\n9  10   يا التاج ع الراس يا السادة يا مالك الروح وراعيها          0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>subtask_a</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>الحمدلله يارب فوز مهم يا زمالك.. كل الدعم ليكم...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>فدوه يا بخت فدوه يا زمن واحد منكم يجيبه</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>RT @USER: يا رب يا واحد يا احد بحق يوم الاحد ا...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>RT @USER: #هوا_الحرية يا وجع قلبي عليكي يا امي...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>يا بكون بحياتك الاهم يا اما ما بدي اكون</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>@USER اخخ يا قلببي يا هالحلقه  متعه على بكاء ع...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>والله الزول السوداني اثبت انه سابق بالتحضر عن ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>RT @USER: جالس اسمع احمد قاسم يغني: \"احبك من ك...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>في قلبي يا مغلاك وبعيني يا محلاك ..</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>يا التاج ع الراس يا السادة يا مالك الروح وراعيها</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PUNCTUATION = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "\n",
    "\n",
    "def remove_emoji(tweet):\n",
    "    return emoji.get_emoji_regexp().sub(u'', tweet)\n",
    "\n",
    "\n",
    "def remove_lf(tweet):\n",
    "    \"\"\" This string appears in many tweets and appears to serve no linguistic function. \"\"\"\n",
    "    return tweet.replace('<LF>', ' ')\n",
    "\n",
    "\n",
    "def remove_repeat_chars(tweet):\n",
    "    \"\"\" Removes only non-digit, non-punctuation characters that are repeated more than once, since double characters can appear in correct spelling. \"\"\"\n",
    "    new = ''\n",
    "\n",
    "    for char in tweet:\n",
    "        if char.isdigit():\n",
    "            new += char\n",
    "        elif char in PUNCTUATION:\n",
    "            new += char\n",
    "        else:\n",
    "            if not new.endswith(char+char):\n",
    "                new += char\n",
    "\n",
    "    return new\n",
    "\n",
    "\n",
    "def remove_diacritics(tweet):\n",
    "    \"\"\" Diacritics in Arabic serve no semantic or syntactic function. \"\"\"\n",
    "    tweet = ar.strip_tashkeel(tweet)\n",
    "    tweet = ar.strip_tatweel(tweet)\n",
    "\n",
    "    tweet = tweet.replace(\"آ\", \"ا\")\n",
    "    tweet = tweet.replace(\"إ\", \"ا\")\n",
    "    tweet = tweet.replace(\"أ\", \"ا\")\n",
    "    tweet = tweet.replace(\"ؤ\", \"و\")\n",
    "    tweet = tweet.replace(\"ئ\", \"ي\")\n",
    "\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def normalise_encoding(tweet):\n",
    "    tweet = re.sub(r'&amp;', '&', tweet)\n",
    "    tweet = unicodedata.normalize('NFC', tweet)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "train['tweet'] = train['tweet'].apply(remove_lf)\n",
    "train['tweet'] = train['tweet'].apply(remove_repeat_chars)\n",
    "train['tweet'] = train['tweet'].apply(remove_diacritics)\n",
    "train['tweet'] = train['tweet'].apply(normalise_encoding)\n",
    "# Convert to binary values\n",
    "train['subtask_a'] = train['subtask_a'].apply(lambda x: 1 if x=='NOT' else 0)\n",
    "\n",
    "# Make one df with emoji and one without to compare performance later\n",
    "train_no_emoji = train\n",
    "train_no_emoji['tweet'] = train_no_emoji['tweet'].apply(remove_emoji)\n",
    "\n",
    "dfs = [train, train_no_emoji]\n",
    "\n",
    "train.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sanity check"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "data": {
      "text/plain": "        id                                              tweet  subtask_a  \\\n2922  2946   من المظاهر الموسفه ان الاب والام  يقعدون يتها...          0   \n6062  6160   جده الحب ،، جده يا منتهى كل الكلام يا سيدة كل...          0   \n4761  4833   يا مالي عمري رضا يا شبيه الورد , يا معنى الحي...          0   \n6564  6707  يا من اظهر الجميل.. وستر القبيح.. يا من لا يوا...          0   \n2753  2760   يا مكعب السكر يا زمردي الاحمر و ياسري الاكبر ...          0   \n2124  2125   اللهم اني اسالك مسالة البايس الفقير وادعوك دع...          1   \n2434  2441  يا سيادة الرييس يا سيادة الرييس نعلم ان الحرب ...          1   \n1399  1400  يا مرحبا ترحيبة عقاب يا نوت والحي يلحق لو تحده...          0   \n3050  3122  انت وامثالك وحكامك وعلمايك الزنادقه خنازير دمر...          1   \n7196  7358  يا عيباه يا حسافاه اليمني بيهان بكل مكان وهذا ...          0   \n\n      char_count  \n2922        5519  \n6062        5198  \n4761        2974  \n6564        2182  \n2753        2043  \n2124        1065  \n2434         284  \n1399         284  \n3050         283  \n7196         283  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>subtask_a</th>\n      <th>char_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2922</th>\n      <td>2946</td>\n      <td>من المظاهر الموسفه ان الاب والام  يقعدون يتها...</td>\n      <td>0</td>\n      <td>5519</td>\n    </tr>\n    <tr>\n      <th>6062</th>\n      <td>6160</td>\n      <td>جده الحب ،، جده يا منتهى كل الكلام يا سيدة كل...</td>\n      <td>0</td>\n      <td>5198</td>\n    </tr>\n    <tr>\n      <th>4761</th>\n      <td>4833</td>\n      <td>يا مالي عمري رضا يا شبيه الورد , يا معنى الحي...</td>\n      <td>0</td>\n      <td>2974</td>\n    </tr>\n    <tr>\n      <th>6564</th>\n      <td>6707</td>\n      <td>يا من اظهر الجميل.. وستر القبيح.. يا من لا يوا...</td>\n      <td>0</td>\n      <td>2182</td>\n    </tr>\n    <tr>\n      <th>2753</th>\n      <td>2760</td>\n      <td>يا مكعب السكر يا زمردي الاحمر و ياسري الاكبر ...</td>\n      <td>0</td>\n      <td>2043</td>\n    </tr>\n    <tr>\n      <th>2124</th>\n      <td>2125</td>\n      <td>اللهم اني اسالك مسالة البايس الفقير وادعوك دع...</td>\n      <td>1</td>\n      <td>1065</td>\n    </tr>\n    <tr>\n      <th>2434</th>\n      <td>2441</td>\n      <td>يا سيادة الرييس يا سيادة الرييس نعلم ان الحرب ...</td>\n      <td>1</td>\n      <td>284</td>\n    </tr>\n    <tr>\n      <th>1399</th>\n      <td>1400</td>\n      <td>يا مرحبا ترحيبة عقاب يا نوت والحي يلحق لو تحده...</td>\n      <td>0</td>\n      <td>284</td>\n    </tr>\n    <tr>\n      <th>3050</th>\n      <td>3122</td>\n      <td>انت وامثالك وحكامك وعلمايك الزنادقه خنازير دمر...</td>\n      <td>1</td>\n      <td>283</td>\n    </tr>\n    <tr>\n      <th>7196</th>\n      <td>7358</td>\n      <td>يا عيباه يا حسافاه اليمني بيهان بكل مكان وهذا ...</td>\n      <td>0</td>\n      <td>283</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_chars(tweet):\n",
    "    return len(tweet)\n",
    "\n",
    "\n",
    "for df in dfs:\n",
    "    df['char_count'] = df['tweet'].apply(count_chars)\n",
    "\n",
    "train_no_emoji.sort_values(by='char_count', ascending=[0]).head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Something is weird with these 6 tweets that have 1000+ characters. Those with 281-284 might be explained by my replacing '\\<LF\\>' with a space to keep word boundaries intact, but 1000+ is clearly some kind of data reading error. I'll just remove them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "data": {
      "text/plain": "        id                                              tweet  subtask_a  \\\n2433  2441  يا سيادة الرييس يا سيادة الرييس نعلم ان الحرب ...          1   \n1399  1400  يا مرحبا ترحيبة عقاب يا نوت والحي يلحق لو تحده...          0   \n7190  7358  يا عيباه يا حسافاه اليمني بيهان بكل مكان وهذا ...          0   \n3047  3122  انت وامثالك وحكامك وعلمايك الزنادقه خنازير دمر...          1   \n6359  6507  بذلة عسكرية وسط الزحام .. حياة مليية بالاشواك ...          0   \n7612  7780  القيناوية غاليين عليا اشرفها مية مية يا خلق يا...          0   \n2653  2661  الهلال امانة في يديكم حنا عند الله ثم عندكم قب...          0   \n6727  6895  يا امعه يا مغفل  تجيب تغريدات مشجعين مراهقين؟ ...          1   \n7734  7902  #قروب_المشتاقون_للجنه #عبدالرحمن_الخشتي اللهم ...          0   \n475    476  الرساله فيها عشم مش طبيعي بس مش مشكلتي. اولا ك...          1   \n\n      char_count  \n2433         284  \n1399         284  \n7190         283  \n3047         283  \n6359         282  \n7612         282  \n2653         282  \n6727         282  \n7734         282  \n475          282  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>subtask_a</th>\n      <th>char_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2433</th>\n      <td>2441</td>\n      <td>يا سيادة الرييس يا سيادة الرييس نعلم ان الحرب ...</td>\n      <td>1</td>\n      <td>284</td>\n    </tr>\n    <tr>\n      <th>1399</th>\n      <td>1400</td>\n      <td>يا مرحبا ترحيبة عقاب يا نوت والحي يلحق لو تحده...</td>\n      <td>0</td>\n      <td>284</td>\n    </tr>\n    <tr>\n      <th>7190</th>\n      <td>7358</td>\n      <td>يا عيباه يا حسافاه اليمني بيهان بكل مكان وهذا ...</td>\n      <td>0</td>\n      <td>283</td>\n    </tr>\n    <tr>\n      <th>3047</th>\n      <td>3122</td>\n      <td>انت وامثالك وحكامك وعلمايك الزنادقه خنازير دمر...</td>\n      <td>1</td>\n      <td>283</td>\n    </tr>\n    <tr>\n      <th>6359</th>\n      <td>6507</td>\n      <td>بذلة عسكرية وسط الزحام .. حياة مليية بالاشواك ...</td>\n      <td>0</td>\n      <td>282</td>\n    </tr>\n    <tr>\n      <th>7612</th>\n      <td>7780</td>\n      <td>القيناوية غاليين عليا اشرفها مية مية يا خلق يا...</td>\n      <td>0</td>\n      <td>282</td>\n    </tr>\n    <tr>\n      <th>2653</th>\n      <td>2661</td>\n      <td>الهلال امانة في يديكم حنا عند الله ثم عندكم قب...</td>\n      <td>0</td>\n      <td>282</td>\n    </tr>\n    <tr>\n      <th>6727</th>\n      <td>6895</td>\n      <td>يا امعه يا مغفل  تجيب تغريدات مشجعين مراهقين؟ ...</td>\n      <td>1</td>\n      <td>282</td>\n    </tr>\n    <tr>\n      <th>7734</th>\n      <td>7902</td>\n      <td>#قروب_المشتاقون_للجنه #عبدالرحمن_الخشتي اللهم ...</td>\n      <td>0</td>\n      <td>282</td>\n    </tr>\n    <tr>\n      <th>475</th>\n      <td>476</td>\n      <td>الرساله فيها عشم مش طبيعي بس مش مشكلتي. اولا ك...</td>\n      <td>1</td>\n      <td>282</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for df in dfs:\n",
    "    df.drop(df[df['char_count'] > 1000].index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train.sort_values(by='char_count', ascending=[0]).head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BERT Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "MAX_LEN = 284\n",
    "\n",
    "def preprocessing_for_bert(data, version=\"mini\"):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-mini-arabic\") if version == \"mini\" else AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n",
    "\n",
    "    # For every sentence...\n",
    "    for i, sent in enumerate(data):\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text = sent,\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,             # Max length to truncate/pad\n",
    "            padding='max_length',           # Pad sentence to max length\n",
    "            return_attention_mask=True,     # Return attention mask\n",
    "            truncation = True\n",
    "            )\n",
    "\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train-test split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tt_split(df):\n",
    "    items, labels = df['tweet'], df['subtask_a']\n",
    "    train_items, test_items, train_labels, test_labels = train_test_split(items, labels, test_size=.15, random_state=42)\n",
    "\n",
    "    train_items = train_items.tolist()\n",
    "    train_labels = train_labels.to_numpy()\n",
    "    test_items = test_items.tolist()\n",
    "    test_labels = test_labels.to_numpy()\n",
    "\n",
    "    return train_items, train_labels, test_items, test_labels\n",
    "\n",
    "\n",
    "data_with_emoji = {}\n",
    "data_without_emoji = {}\n",
    "\n",
    "data_with_emoji['train items'], data_with_emoji['train labels'], data_with_emoji['test items'], data_with_emoji['test labels'] \\\n",
    "    = tt_split(train)\n",
    "data_without_emoji['train items'], data_without_emoji['train labels'], data_without_emoji['test items'], data_without_emoji['test labels'] \\\n",
    "    = tt_split(train_no_emoji)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "dicts = [data_with_emoji, data_without_emoji]\n",
    "\n",
    "# Run function `preprocessing_for_bert` on the train and test sets and convert label types to torch.Tensor\n",
    "for d in dicts:\n",
    "    d['train input ids'], d['train masks'] = preprocessing_for_bert(d['train items'])\n",
    "    d['test input ids'], d['test masks'] = preprocessing_for_bert(d['test items'])\n",
    "\n",
    "    d['train labels'] = torch.tensor(d['train labels'])\n",
    "    d['test labels'] = torch.tensor(d['test labels'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 16\n",
    "\n",
    "## Create the DataLoader for our training set\n",
    "# With emoji:\n",
    "train_data_emoji = TensorDataset(data_with_emoji['train input ids'], data_with_emoji['train masks'], data_with_emoji['train labels'])\n",
    "train_sampler_emoji = RandomSampler(train_data_emoji)\n",
    "train_dataloader_emoji = DataLoader(train_data_emoji, sampler=train_sampler_emoji, batch_size=batch_size)\n",
    "# Without emoji:\n",
    "train_data_no_emoji = TensorDataset(data_without_emoji['train input ids'], data_without_emoji['train masks'], data_without_emoji['train labels'])\n",
    "train_sampler_no_emoji = RandomSampler(train_data_no_emoji)\n",
    "train_dataloader_no_emoji = DataLoader(train_data_no_emoji, sampler=train_sampler_no_emoji, batch_size=batch_size)\n",
    "\n",
    "## Create the DataLoader for our test set\n",
    "# With emoji:\n",
    "test_data_emoji = TensorDataset(data_with_emoji['test input ids'], data_with_emoji['test masks'], data_with_emoji['test labels'])\n",
    "test_sampler_emoji = RandomSampler(test_data_emoji)\n",
    "test_dataloader_emoji = DataLoader(test_data_emoji, sampler=test_sampler_emoji, batch_size=batch_size)\n",
    "# Without emoji:\n",
    "test_data_no_emoji = TensorDataset(data_without_emoji['test input ids'], data_without_emoji['test masks'], data_without_emoji['test labels'])\n",
    "test_sampler_no_emoji = RandomSampler(test_data_no_emoji)\n",
    "test_dataloader_no_emoji = DataLoader(test_data_no_emoji, sampler=test_sampler_no_emoji, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "# Create the BertClassifier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\" Bert Model for Classification Tasks. \"\"\"\n",
    "\n",
    "    def __init__(self, freeze_bert=False, version=\"mini\"):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in = 256 if version == \"mini\" else 768\n",
    "        H, D_out = 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = AutoModel.from_pretrained(\"asafaya/bert-mini-arabic\") if version == \"mini\" else AutoModel.from_pretrained(\"asafaya/bert-base-arabic\")\n",
    "        # Instantiate a single-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that holds attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "def initialize_model(train_dataloader, epochs=4, version=\"mini\"):\n",
    "    \"\"\" Initialize the Bert Classifier, the optimizer and the learning rate scheduler. \"\"\"\n",
    "\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False, version=version)\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(params=list(bert_classifier.parameters()),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "\n",
    "    return bert_classifier, optimizer, scheduler\n",
    "\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\" Set seed for reproducibility. \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at asafaya/bert-mini-arabic were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/Jessie/Downloads/UZH/text_mining/Assignments/ex_05/Shared_Task/venv/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at asafaya/bert-mini-arabic were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_dataloader, test_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\" Train the BertClassifier model. \"\"\"\n",
    "\n",
    "    # Start training loop\n",
    "    print(\"Starting training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Test Loss':^10} | {'Test Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts += 1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation:\n",
    "            # After the completion of each training epoch, measure the model's performance on our test set.\n",
    "            val_loss, val_accuracy = evaluate(model, test_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "\n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\" After the completion of each training epoch, measure the model's performance on our test set. \"\"\"\n",
    "\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the test set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "# With emoji:\n",
    "bert_classifier_emoji, optimizer, scheduler = initialize_model(train_dataloader_emoji, epochs=2)\n",
    "# Without emoji:\n",
    "bert_classifier_no_emoji, optimizer, scheduler = initialize_model(train_dataloader_no_emoji, epochs=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH EMOJI:\n",
      "Starting training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.824876   |     -      |     -     |   55.57  \n",
      "   1    |   40    |   0.824565   |     -      |     -     |   52.92  \n",
      "   1    |   60    |   0.828333   |     -      |     -     |   53.25  \n",
      "   1    |   80    |   0.836187   |     -      |     -     |   53.28  \n",
      "   1    |   100   |   0.809874   |     -      |     -     |   53.79  \n",
      "   1    |   120   |   0.856051   |     -      |     -     |   53.87  \n",
      "   1    |   140   |   0.860695   |     -      |     -     |   53.67  \n",
      "   1    |   160   |   0.838221   |     -      |     -     |   53.71  \n",
      "   1    |   180   |   0.839605   |     -      |     -     |   53.34  \n",
      "   1    |   200   |   0.820375   |     -      |     -     |   53.43  \n",
      "   1    |   220   |   0.821903   |     -      |     -     |   53.57  \n",
      "   1    |   240   |   0.853965   |     -      |     -     |   53.63  \n",
      "   1    |   260   |   0.814622   |     -      |     -     |   53.56  \n",
      "   1    |   280   |   0.857140   |     -      |     -     |   53.53  \n",
      "   1    |   300   |   0.848585   |     -      |     -     |   53.59  \n",
      "   1    |   320   |   0.847220   |     -      |     -     |   53.33  \n",
      "   1    |   340   |   0.806389   |     -      |     -     |   53.38  \n",
      "   1    |   360   |   0.841915   |     -      |     -     |   53.41  \n",
      "   1    |   380   |   0.839397   |     -      |     -     |   53.51  \n",
      "   1    |   400   |   0.826280   |     -      |     -     |   53.42  \n",
      "   1    |   416   |   0.796027   |     -      |     -     |   40.28  \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.833298   |  0.807730  |   21.48   |  1179.38 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.832862   |     -      |     -     |   56.08  \n",
      "   2    |   40    |   0.841472   |     -      |     -     |   53.40  \n",
      "   2    |   60    |   0.844153   |     -      |     -     |   53.34  \n",
      "   2    |   80    |   0.851959   |     -      |     -     |   53.64  \n",
      "   2    |   100   |   0.843177   |     -      |     -     |   53.56  \n",
      "   2    |   120   |   0.832197   |     -      |     -     |   53.54  \n",
      "   2    |   140   |   0.821876   |     -      |     -     |   53.47  \n",
      "   2    |   160   |   0.861972   |     -      |     -     |   53.67  \n",
      "   2    |   180   |   0.846404   |     -      |     -     |   53.36  \n",
      "   2    |   200   |   0.826928   |     -      |     -     |   53.37  \n",
      "   2    |   220   |   0.841400   |     -      |     -     |   53.42  \n",
      "   2    |   240   |   0.827558   |     -      |     -     |   53.50  \n",
      "   2    |   260   |   0.839428   |     -      |     -     |   53.29  \n",
      "   2    |   280   |   0.850544   |     -      |     -     |   57.52  \n",
      "   2    |   300   |   0.824928   |     -      |     -     |   53.23  \n",
      "   2    |   320   |   0.857931   |     -      |     -     |   53.22  \n",
      "   2    |   340   |   0.841512   |     -      |     -     |   53.24  \n",
      "   2    |   360   |   0.827756   |     -      |     -     |   53.22  \n",
      "   2    |   380   |   0.842725   |     -      |     -     |   53.22  \n",
      "   2    |   400   |   0.842198   |     -      |     -     |   53.19  \n",
      "   2    |   416   |   0.835208   |     -      |     -     |   40.40  \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.839750   |  0.808326  |   21.48   |  1182.18 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "print('WITH EMOJI:')\n",
    "train(bert_classifier_emoji, train_dataloader_emoji, test_dataloader_emoji, epochs=2, evaluation=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHOUT EMOJI:\n",
      "Starting training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.782170   |     -      |     -     |   56.10  \n",
      "   1    |   40    |   0.770123   |     -      |     -     |   53.66  \n",
      "   1    |   60    |   0.788341   |     -      |     -     |   53.95  \n",
      "   1    |   80    |   0.788008   |     -      |     -     |   54.06  \n",
      "   1    |   100   |   0.781266   |     -      |     -     |   53.85  \n",
      "   1    |   120   |   0.774466   |     -      |     -     |   54.11  \n",
      "   1    |   140   |   0.763106   |     -      |     -     |   53.84  \n",
      "   1    |   160   |   0.775399   |     -      |     -     |   53.91  \n",
      "   1    |   180   |   0.756037   |     -      |     -     |   53.93  \n",
      "   1    |   200   |   0.775781   |     -      |     -     |   55.18  \n",
      "   1    |   220   |   0.785766   |     -      |     -     |   54.19  \n",
      "   1    |   240   |   0.770708   |     -      |     -     |   53.86  \n",
      "   1    |   260   |   0.767178   |     -      |     -     |   54.00  \n",
      "   1    |   280   |   0.770719   |     -      |     -     |   53.75  \n",
      "   1    |   300   |   0.762126   |     -      |     -     |   53.80  \n",
      "   1    |   320   |   0.755916   |     -      |     -     |   53.77  \n",
      "   1    |   340   |   0.775496   |     -      |     -     |   53.77  \n",
      "   1    |   360   |   0.764208   |     -      |     -     |   53.75  \n",
      "   1    |   380   |   0.791124   |     -      |     -     |   53.87  \n",
      "   1    |   400   |   0.785044   |     -      |     -     |   53.84  \n",
      "   1    |   416   |   0.785598   |     -      |     -     |   40.83  \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.774608   |  0.761867  |   28.08   |  1189.03 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.771291   |     -      |     -     |   56.44  \n",
      "   2    |   40    |   0.775153   |     -      |     -     |   53.87  \n",
      "   2    |   60    |   0.764138   |     -      |     -     |   53.76  \n",
      "   2    |   80    |   0.775073   |     -      |     -     |   53.76  \n",
      "   2    |   100   |   0.775289   |     -      |     -     |   53.72  \n",
      "   2    |   120   |   0.785751   |     -      |     -     |   53.64  \n",
      "   2    |   140   |   0.774265   |     -      |     -     |   53.62  \n",
      "   2    |   160   |   0.775942   |     -      |     -     |   53.73  \n",
      "   2    |   180   |   0.803314   |     -      |     -     |   53.64  \n",
      "   2    |   200   |   0.764122   |     -      |     -     |   53.60  \n",
      "   2    |   220   |   0.763728   |     -      |     -     |   55.03  \n",
      "   2    |   240   |   0.775613   |     -      |     -     |   53.76  \n",
      "   2    |   260   |   0.771816   |     -      |     -     |   53.76  \n",
      "   2    |   280   |   0.777722   |     -      |     -     |   53.81  \n",
      "   2    |   300   |   0.773998   |     -      |     -     |   53.74  \n",
      "   2    |   320   |   0.792314   |     -      |     -     |   53.80  \n",
      "   2    |   340   |   0.768734   |     -      |     -     |   53.74  \n",
      "   2    |   360   |   0.789747   |     -      |     -     |   53.72  \n",
      "   2    |   380   |   0.794050   |     -      |     -     |   53.72  \n",
      "   2    |   400   |   0.761003   |     -      |     -     |   53.76  \n",
      "   2    |   416   |   0.786071   |     -      |     -     |   40.93  \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.777002   |  0.763002  |   27.64   |  1186.55 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "print('WITHOUT EMOJI:')\n",
    "train(bert_classifier_no_emoji, train_dataloader_no_emoji, test_dataloader_no_emoji, epochs=2, evaluation=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [
    "# Saving the model for future runs\n",
    "pickle.dump(bert_classifier_emoji, open('trained_model_mini_with_emoji.sav', 'wb'))\n",
    "pickle.dump(bert_classifier_no_emoji, open('trained_model_mini_without_emoji.sav', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [],
   "source": [
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\" Perform a forward pass on the trained BERT model to predict probabilities on the test set. \"\"\"\n",
    "\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "\n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluating"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "def calculate_prec_acc(probs, y_true):\n",
    "    preds = probs[:, 1]\n",
    "\n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "\n",
    "    print(f'Accuracy: {accuracy_score(y_true, y_pred) * 100:.2f}%')\n",
    "    print(f'Precision: {precision_score(y_true, y_pred) * 100:.2f}%')\n",
    "    print(f'Recall: {recall_score(y_true, y_pred) * 100:.2f}%')\n",
    "    print(f'f1-score: {f1_score(y_true, y_pred) * 100:.2f}%')\n",
    "\n",
    "\n",
    "# Compute predicted probabilities on the test set\n",
    "probs_emoji = bert_predict(bert_classifier_emoji, test_dataloader_emoji)\n",
    "probs_no_emoji = bert_predict(bert_classifier_no_emoji, test_dataloader_no_emoji)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--WITH EMOJI--\n",
      "\n",
      "Accuracy: 21.70%\n",
      "Precision: 20.05%\n",
      "Recall: 98.71%\n",
      "f1-score: 33.33%\n"
     ]
    }
   ],
   "source": [
    "print('--WITH EMOJI--\\n')\n",
    "calculate_prec_acc(probs_emoji, data_with_emoji['test labels'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--WITHOUT EMOJI--\n",
      "\n",
      "Accuracy: 29.62%\n",
      "Precision: 19.13%\n",
      "Recall: 78.97%\n",
      "f1-score: 30.79%\n"
     ]
    }
   ],
   "source": [
    "print('--WITHOUT EMOJI--\\n')\n",
    "calculate_prec_acc(probs_no_emoji, data_without_emoji['test labels'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# removing emojis \n",
    "def remove_emoji(string):\n",
    "    return emoji.get_emoji_regexp().sub(u'', string)\n",
    "\n",
    "# removing USER\n",
    "def user_remove(string):\n",
    "  string = re.sub('(USER)', '', string)\n",
    "  return string\n",
    "train['tweet'] = train['tweet'].apply(remove_emoji)\n",
    "train['tweet'] = train['tweet'].apply(user_remove)\n",
    "train.head(40)"
   ],
   "metadata": {
    "id": "GlBslXmWFYJW",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "02436ba9-4a99-478a-f022-c47816585180"
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: 'emoji.get_emoji_regexp()' is deprecated and will be removed in version 2.0.0. If you want to remove emoji from a string, consider the method emoji.replace_emoji(str, replace='').\n",
      "To hide this warning, pin/downgrade the package to 'emoji~=1.6.3'\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        id                                              tweet subtask_a  \\\n",
       "2922  2946  LFمن المظاهر المءسفه ان الاب والام LFيقعدون يت...       NOT   \n",
       "6062  6160  LFجده الحب LFجده منتهي الكلام سيده المدن  URL ...       NOT   \n",
       "4761  4833  مالي عمري رضا شبيه الورد معني الحياه NOT 4834 ...       NOT   \n",
       "6564  6707  اظهر الجميل وستر القبيح يءاخذ بالجريره يهتك ال...       NOT   \n",
       "2753  2760  مكعب السكر زمردي الاحمر ياسري الاكبر فتنتي الع...       NOT   \n",
       "2124  2125  اللهم اني اسالك مساله الباءس الفقير وادعوك دعا...       OFF   \n",
       "5316  5414  رنا رنا رنا رنا رنا رنا رنا رنا رنا رنا رنا رن...       NOT   \n",
       "6428  6571  باي باي باي باي باي باي باي باي باي باي باي با...       NOT   \n",
       "4030  4102  فهد باص فهد باص فهد باص فهد باص فهد باص فهد با...       OFF   \n",
       "3777  3849  رب اتسفه رب اتسفه رب اتسفه رب اتسفه رب اتسفه ر...       NOT   \n",
       "7761  7923  رب انجلد رب انجلد رب انجلد رب انجلد رب انجلد ر...       NOT   \n",
       "2297  2304  يا الله  الله  الله  الله  الله  الله يا الله ...       NOT   \n",
       "670    671  دعاء سيدنا يوسف الجب LFاللهم مءنس غريب ويا صاح...       NOT   \n",
       "3471  3543  تابع ادعووقليا كافي الاسواء شافي الادواء جبار ...       NOT   \n",
       "3022  3094  الله ياخذك جاهل تعبان مريض معتوه ياخذ رامي عيس...       OFF   \n",
       "3045  3117  تطمعين ساره تطمعين ساره LFلا تطمعين ساره تطمعي...       NOT   \n",
       "5442  5540  الاتحادالنصرLFيا رب نفوز رب نفوز رب نفوز رب نف...       NOT   \n",
       "2701  2708   زينب زينب زينب زينب زينب زينب زينب زينب زينب ...       NOT   \n",
       "77      78  صاحباتي اقاربي اللي عمري عايشين مستقرين برا ال...       NOT   \n",
       "1630  1631  ياحي يقيوم رحمتك است غيث اصلح شاني كله تكلني ا...       NOT   \n",
       "5883  5981  مالك دخل روان LFمالك دخل روان LFمالك دخل روان ...       NOT   \n",
       "3320  3392  رب ادرس رب ادرس LFيا رب ادرس رب ادرس LFيا رب ا...       NOT   \n",
       "1594  1595  رب اليوڤي يتاهل رب اليوڤي يتاهل رب اليوڤي يتاه...       NOT   \n",
       "3642  3714  ويكند بريال ونص تقريبا الان سيكون 24 ساعه الخي...       OFF   \n",
       "6640  6802  ادري اني متاخره كثير بالي كنتي والله نسيتك مرر...       NOT   \n",
       "723    724  اظهر الجميل ستر القبيح يءاخذ بالجريره يهتك الس...       NOT   \n",
       "6210  6353  يحبك رواء يحبك رواء يحبك رواء يحبك رواء يحبك ر...       NOT   \n",
       "5285  5383  اسكت لساني اسكت لساني اسكت لساني اسكت لساني اس...       NOT   \n",
       "6949  7111  اذا دعي اجاب سريع الحساب رب الارباب عظيم الجنا...       NOT   \n",
       "5802  5900  عليا اليمين والطلاق الوليه الي مزال ماجت ومانض...       OFF   \n",
       "3420  3492  كلنا عيوب الا ستر ربنا علينا LFربنا يسترك يستر...       NOT   \n",
       "4714  4786  اليمني قحبه فاسد استغلالك لدين فساد باسم الدين...       OFF   \n",
       "3121  3193  سقيحبكشعبانالحب LFالذين دعواهم سبحانك اللهم وت...       NOT   \n",
       "7461  7623  اللهم يعيد للمريض صحته ويستجيب دعاء عباده المو...       NOT   \n",
       "5559  5657  اللهم مغفرتك اوسع ذنوبي ورحمتك ارجي عندي عملي ...       NOT   \n",
       "4940  5038  جماعه والله احنا عارفين اننا متغيرين شكلنا تعب...       NOT   \n",
       "218    219  اللهم صل وسلم علي سيدنا محمد وعلي ال سيدنا محم...       NOT   \n",
       "6284  6427  بالي شيءين احسهم فرض علي شخص تويتر مو مخير فيه...       NOT   \n",
       "7478  7640  اهلي نصف ربعي ياخديني LFيا مواني غربتي ياروح د...       NOT   \n",
       "3491  3563  قال ﷺ قال لاخيه عدو الله او قال كافر فقد احدهم...       NOT   \n",
       "\n",
       "      word_count  char_count  avg_char_per_word  stopwords  emoji_count  \n",
       "2922         941        5857           4.403137        211           74  \n",
       "6062         862        5416           4.437751        203           36  \n",
       "4761         531        3178           4.226974        147          108  \n",
       "6564         367        2283           4.397163        102            8  \n",
       "2753         363        2129           4.165049         79           34  \n",
       "2124         179        1113           4.673469         36            3  \n",
       "5316          84         279           2.333333         56            0  \n",
       "6428          78         272           2.500000         39            0  \n",
       "4030          74         276           2.743243         24            0  \n",
       "3777          73         278           2.821918         36            0  \n",
       "7761          73         271           2.726027         36            0  \n",
       "2297          69         247           2.594203         20           32  \n",
       "670           64         272           3.265625         21            0  \n",
       "3471          63         275           3.380952         26            0  \n",
       "3022          62         282           3.564516         19            2  \n",
       "3045          62         283           3.580645         30            0  \n",
       "5442          61         259           3.262295         15            4  \n",
       "2701          61         244           3.016393         30            0  \n",
       "77            61         279           3.590164         20            0  \n",
       "1630          60         279           3.666667         20            0  \n",
       "5883          60         315           4.266667         15            0  \n",
       "3320          60         255           3.266667         11            0  \n",
       "1594          60         280           3.683333         16            0  \n",
       "3642          60         280           3.683333         17            0  \n",
       "6640          60         277           3.633333         15            0  \n",
       "723           60         277           3.633333         26            0  \n",
       "6210          60         255           3.266667         30            0  \n",
       "5285          60         279           3.666667         20            0  \n",
       "6949          59         276           3.694915         14            0  \n",
       "5802          59         275           3.677966         23            0  \n",
       "3420          59         263           3.474576         17            0  \n",
       "4714          58         262           3.534483         22            0  \n",
       "3121          58         274           3.741379         19            0  \n",
       "7461          58         271           3.689655         18            0  \n",
       "5559          58         246           3.258621         12            0  \n",
       "4940          58         279           3.827586         11            0  \n",
       "218           58         264           3.568966         12            3  \n",
       "6284          58         280           3.844828         12            0  \n",
       "7478          58         298           4.155172         12            1  \n",
       "3491          57         256           3.508772         25            0  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-130f3991-6d10-4199-89bd-91e20808a325\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_char_per_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>emoji_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2922</th>\n",
       "      <td>2946</td>\n",
       "      <td>LFمن المظاهر المءسفه ان الاب والام LFيقعدون يت...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>941</td>\n",
       "      <td>5857</td>\n",
       "      <td>4.403137</td>\n",
       "      <td>211</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6062</th>\n",
       "      <td>6160</td>\n",
       "      <td>LFجده الحب LFجده منتهي الكلام سيده المدن  URL ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>862</td>\n",
       "      <td>5416</td>\n",
       "      <td>4.437751</td>\n",
       "      <td>203</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4761</th>\n",
       "      <td>4833</td>\n",
       "      <td>مالي عمري رضا شبيه الورد معني الحياه NOT 4834 ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>531</td>\n",
       "      <td>3178</td>\n",
       "      <td>4.226974</td>\n",
       "      <td>147</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6564</th>\n",
       "      <td>6707</td>\n",
       "      <td>اظهر الجميل وستر القبيح يءاخذ بالجريره يهتك ال...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>367</td>\n",
       "      <td>2283</td>\n",
       "      <td>4.397163</td>\n",
       "      <td>102</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2753</th>\n",
       "      <td>2760</td>\n",
       "      <td>مكعب السكر زمردي الاحمر ياسري الاكبر فتنتي الع...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>363</td>\n",
       "      <td>2129</td>\n",
       "      <td>4.165049</td>\n",
       "      <td>79</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2124</th>\n",
       "      <td>2125</td>\n",
       "      <td>اللهم اني اسالك مساله الباءس الفقير وادعوك دعا...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>179</td>\n",
       "      <td>1113</td>\n",
       "      <td>4.673469</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>5414</td>\n",
       "      <td>رنا رنا رنا رنا رنا رنا رنا رنا رنا رنا رنا رن...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>84</td>\n",
       "      <td>279</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6428</th>\n",
       "      <td>6571</td>\n",
       "      <td>باي باي باي باي باي باي باي باي باي باي باي با...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>78</td>\n",
       "      <td>272</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4030</th>\n",
       "      <td>4102</td>\n",
       "      <td>فهد باص فهد باص فهد باص فهد باص فهد باص فهد با...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>74</td>\n",
       "      <td>276</td>\n",
       "      <td>2.743243</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3777</th>\n",
       "      <td>3849</td>\n",
       "      <td>رب اتسفه رب اتسفه رب اتسفه رب اتسفه رب اتسفه ر...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>73</td>\n",
       "      <td>278</td>\n",
       "      <td>2.821918</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7761</th>\n",
       "      <td>7923</td>\n",
       "      <td>رب انجلد رب انجلد رب انجلد رب انجلد رب انجلد ر...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>73</td>\n",
       "      <td>271</td>\n",
       "      <td>2.726027</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>2304</td>\n",
       "      <td>يا الله  الله  الله  الله  الله  الله يا الله ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>69</td>\n",
       "      <td>247</td>\n",
       "      <td>2.594203</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>671</td>\n",
       "      <td>دعاء سيدنا يوسف الجب LFاللهم مءنس غريب ويا صاح...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>64</td>\n",
       "      <td>272</td>\n",
       "      <td>3.265625</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3471</th>\n",
       "      <td>3543</td>\n",
       "      <td>تابع ادعووقليا كافي الاسواء شافي الادواء جبار ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>63</td>\n",
       "      <td>275</td>\n",
       "      <td>3.380952</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>3094</td>\n",
       "      <td>الله ياخذك جاهل تعبان مريض معتوه ياخذ رامي عيس...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>62</td>\n",
       "      <td>282</td>\n",
       "      <td>3.564516</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3045</th>\n",
       "      <td>3117</td>\n",
       "      <td>تطمعين ساره تطمعين ساره LFلا تطمعين ساره تطمعي...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>62</td>\n",
       "      <td>283</td>\n",
       "      <td>3.580645</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5442</th>\n",
       "      <td>5540</td>\n",
       "      <td>الاتحادالنصرLFيا رب نفوز رب نفوز رب نفوز رب نف...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>61</td>\n",
       "      <td>259</td>\n",
       "      <td>3.262295</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <td>2708</td>\n",
       "      <td>زينب زينب زينب زينب زينب زينب زينب زينب زينب ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>61</td>\n",
       "      <td>244</td>\n",
       "      <td>3.016393</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>صاحباتي اقاربي اللي عمري عايشين مستقرين برا ال...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>61</td>\n",
       "      <td>279</td>\n",
       "      <td>3.590164</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>1631</td>\n",
       "      <td>ياحي يقيوم رحمتك است غيث اصلح شاني كله تكلني ا...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>60</td>\n",
       "      <td>279</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5883</th>\n",
       "      <td>5981</td>\n",
       "      <td>مالك دخل روان LFمالك دخل روان LFمالك دخل روان ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>60</td>\n",
       "      <td>315</td>\n",
       "      <td>4.266667</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3320</th>\n",
       "      <td>3392</td>\n",
       "      <td>رب ادرس رب ادرس LFيا رب ادرس رب ادرس LFيا رب ا...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>60</td>\n",
       "      <td>255</td>\n",
       "      <td>3.266667</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>1595</td>\n",
       "      <td>رب اليوڤي يتاهل رب اليوڤي يتاهل رب اليوڤي يتاه...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>60</td>\n",
       "      <td>280</td>\n",
       "      <td>3.683333</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3642</th>\n",
       "      <td>3714</td>\n",
       "      <td>ويكند بريال ونص تقريبا الان سيكون 24 ساعه الخي...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>60</td>\n",
       "      <td>280</td>\n",
       "      <td>3.683333</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6640</th>\n",
       "      <td>6802</td>\n",
       "      <td>ادري اني متاخره كثير بالي كنتي والله نسيتك مرر...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>60</td>\n",
       "      <td>277</td>\n",
       "      <td>3.633333</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>724</td>\n",
       "      <td>اظهر الجميل ستر القبيح يءاخذ بالجريره يهتك الس...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>60</td>\n",
       "      <td>277</td>\n",
       "      <td>3.633333</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6210</th>\n",
       "      <td>6353</td>\n",
       "      <td>يحبك رواء يحبك رواء يحبك رواء يحبك رواء يحبك ر...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>60</td>\n",
       "      <td>255</td>\n",
       "      <td>3.266667</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5285</th>\n",
       "      <td>5383</td>\n",
       "      <td>اسكت لساني اسكت لساني اسكت لساني اسكت لساني اس...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>60</td>\n",
       "      <td>279</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6949</th>\n",
       "      <td>7111</td>\n",
       "      <td>اذا دعي اجاب سريع الحساب رب الارباب عظيم الجنا...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>59</td>\n",
       "      <td>276</td>\n",
       "      <td>3.694915</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5802</th>\n",
       "      <td>5900</td>\n",
       "      <td>عليا اليمين والطلاق الوليه الي مزال ماجت ومانض...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>59</td>\n",
       "      <td>275</td>\n",
       "      <td>3.677966</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3420</th>\n",
       "      <td>3492</td>\n",
       "      <td>كلنا عيوب الا ستر ربنا علينا LFربنا يسترك يستر...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>59</td>\n",
       "      <td>263</td>\n",
       "      <td>3.474576</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4714</th>\n",
       "      <td>4786</td>\n",
       "      <td>اليمني قحبه فاسد استغلالك لدين فساد باسم الدين...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>58</td>\n",
       "      <td>262</td>\n",
       "      <td>3.534483</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3121</th>\n",
       "      <td>3193</td>\n",
       "      <td>سقيحبكشعبانالحب LFالذين دعواهم سبحانك اللهم وت...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>58</td>\n",
       "      <td>274</td>\n",
       "      <td>3.741379</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7461</th>\n",
       "      <td>7623</td>\n",
       "      <td>اللهم يعيد للمريض صحته ويستجيب دعاء عباده المو...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>58</td>\n",
       "      <td>271</td>\n",
       "      <td>3.689655</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5559</th>\n",
       "      <td>5657</td>\n",
       "      <td>اللهم مغفرتك اوسع ذنوبي ورحمتك ارجي عندي عملي ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>58</td>\n",
       "      <td>246</td>\n",
       "      <td>3.258621</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4940</th>\n",
       "      <td>5038</td>\n",
       "      <td>جماعه والله احنا عارفين اننا متغيرين شكلنا تعب...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>58</td>\n",
       "      <td>279</td>\n",
       "      <td>3.827586</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>219</td>\n",
       "      <td>اللهم صل وسلم علي سيدنا محمد وعلي ال سيدنا محم...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>58</td>\n",
       "      <td>264</td>\n",
       "      <td>3.568966</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6284</th>\n",
       "      <td>6427</td>\n",
       "      <td>بالي شيءين احسهم فرض علي شخص تويتر مو مخير فيه...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>58</td>\n",
       "      <td>280</td>\n",
       "      <td>3.844828</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7478</th>\n",
       "      <td>7640</td>\n",
       "      <td>اهلي نصف ربعي ياخديني LFيا مواني غربتي ياروح د...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>58</td>\n",
       "      <td>298</td>\n",
       "      <td>4.155172</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3491</th>\n",
       "      <td>3563</td>\n",
       "      <td>قال ﷺ قال لاخيه عدو الله او قال كافر فقد احدهم...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>57</td>\n",
       "      <td>256</td>\n",
       "      <td>3.508772</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-130f3991-6d10-4199-89bd-91e20808a325')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-130f3991-6d10-4199-89bd-91e20808a325 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-130f3991-6d10-4199-89bd-91e20808a325');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = train\n",
    "X = data.tweet.values\n",
    "y = data.subtask_a.values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=2020)\n"
   ],
   "metadata": {
    "id": "mwAnkKG1PByU"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}