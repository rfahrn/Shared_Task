{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jroady_rfahrn_tm_ex05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "3Sup6WXLW-NI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install emoji\n",
        "! pip install pyarabic\n",
        "! pip install transformers"
      ],
      "metadata": {
        "id": "I-uHeaF8XN-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd93c762-b641-499e-aedf-933ec820c413"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.7.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.7/dist-packages (0.6.15)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from pyarabic) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/rfahrn/Shared_Task.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9fy7pyiX-8r",
        "outputId": "3a99151d-3ad3-4289-de82-9a3c295749fe"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Shared_Task' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import emoji\n",
        "import re\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pyarabic.araby as ar\n",
        "import pickle\n",
        "import string\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Z7y8PacKWVWo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-mini-arabic\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "cnCqwexiWVWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45271d42-2b0b-491c-c9a1-f4c1b84807a3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "source": [
        "train = pd.read_csv('Shared_Task/data/offenseval-ar-training-v1.tsv', sep='\\t', encoding='utf-8')"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "nJM1mgRNWVWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preview data"
      ],
      "metadata": {
        "id": "ngu1Lp3IaIbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(10)"
      ],
      "metadata": {
        "id": "57aoLZH8aF3X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "2b45ee7c-6c56-4d23-dfae-3f3c2ac082dd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                              tweet subtask_a\n",
              "0   1  الحمدلله يارب فوز مهم يا زمالك.. كل الدعم ليكم...       NOT\n",
              "1   2            فدوه يا بخت فدوه يا زمن واحد منكم يجيبه       NOT\n",
              "2   3  RT @USER: يا رب يا واحد يا أحد بحق يوم الاحد ا...       OFF\n",
              "3   4  RT @USER: #هوا_الحرية يا وجع قلبي عليكي يا امي...       NOT\n",
              "4   5          يا بكون بحياتك الأهم يا إما ما بدي أكون 🎼       NOT\n",
              "5   6  @USER اخخ يا قلببي يا هالحلقه 😩😭♥️ متعه على بك...       NOT\n",
              "6   7  والله الزول السوداني أثبت أنه سابق بالتحضّر عن...       NOT\n",
              "7   8  RT @USER: جالس أسمع أحمد قاسم يغني: \"أحبك من ك...       NOT\n",
              "8   9             في قلبي يا مغلاك<LF>وبعيني يا محلاك ..       NOT\n",
              "9  10  يا التاج ع الراس يا السادة<LF>يا مالك الروح ور...       NOT"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e8d530c3-d06d-49f1-b9c9-c1a39f1859e2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>subtask_a</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>الحمدلله يارب فوز مهم يا زمالك.. كل الدعم ليكم...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>فدوه يا بخت فدوه يا زمن واحد منكم يجيبه</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>RT @USER: يا رب يا واحد يا أحد بحق يوم الاحد ا...</td>\n",
              "      <td>OFF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>RT @USER: #هوا_الحرية يا وجع قلبي عليكي يا امي...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>يا بكون بحياتك الأهم يا إما ما بدي أكون 🎼</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>@USER اخخ يا قلببي يا هالحلقه 😩😭♥️ متعه على بك...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>والله الزول السوداني أثبت أنه سابق بالتحضّر عن...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>RT @USER: جالس أسمع أحمد قاسم يغني: \"أحبك من ك...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>في قلبي يا مغلاك&lt;LF&gt;وبعيني يا محلاك ..</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>يا التاج ع الراس يا السادة&lt;LF&gt;يا مالك الروح ور...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8d530c3-d06d-49f1-b9c9-c1a39f1859e2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e8d530c3-d06d-49f1-b9c9-c1a39f1859e2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e8d530c3-d06d-49f1-b9c9-c1a39f1859e2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(train.subtask_a)\n",
        "plt.title('Count NOT/OFF')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "4pLDsSenaPbM",
        "outputId": "b47cb710-8eff-408c-dd31-35b89aa48fa2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Count NOT/OFF')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train['subtask_a'].value_counts(), '\\n')\n",
        "\n",
        "print(f\"Total number of tweets: {len(train)}\")\n",
        "print(f\"% non-offensive: {6289/7839}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KazxYBaEZFip",
        "outputId": "d48a5c76-b55b-4407-ce5d-d1c2f3b2b568"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NOT    6289\n",
            "OFF    1550\n",
            "Name: subtask_a, dtype: int64 \n",
            "\n",
            "Total number of tweets: 7839\n",
            "% non-offensive: 0.8022706977930859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there are much more non-offensive than offensive tweets - about a 80:20 ratio. This is a common issue in HSD data. Transformers tend to perform better with balanced datasets."
      ],
      "metadata": {
        "id": "obXvw4RsXcaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing"
      ],
      "metadata": {
        "id": "d7Yox42IXndS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Arabic + English punctuation\n",
        "PUNCTUATION = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n",
        "\n",
        "\n",
        "def remove_emoji(tweet):\n",
        "  return emoji.get_emoji_regexp().sub(u' ', tweet)\n",
        "\n",
        "\n",
        "def remove_lf(tweet):\n",
        "  \"\"\" This string appears in many tweets and appears to serve no linguistic function. \"\"\"\n",
        "  return tweet.replace('<LF>', ' ')\n",
        "  \n",
        "\n",
        "def remove_repeat_chars(tweet):\n",
        "  \"\"\" Removes only non-digit, non-punctuation characters that are repeated more than once, since double characters can appear in correct spelling. \"\"\"\n",
        "  new = ''\n",
        "\n",
        "  for char in tweet:\n",
        "      if char.isdigit():\n",
        "          new += char\n",
        "      elif char in PUNCTUATION:\n",
        "          new += char\n",
        "      else:\n",
        "          if not new.endswith(char+char):\n",
        "              new += char\n",
        "\n",
        "  return new\n",
        "\n",
        "\n",
        "def normalise_encoding(tweet):\n",
        "  tweet = re.sub(r'&amp;', '&', tweet)\n",
        "  tweet = re.sub(r'\\\\u....', ' ', tweet)  # Remove the \\uXXXX that ends up replacing left-to-right space characters\n",
        "  tweet = re.sub(r'\\\\xa0', ' ', tweet)  # Remove the \\xa0 that ends up replacing NBSP characters\n",
        "  return tweet\n",
        "\n",
        "\n",
        "def remove_diacritics(tweet):\n",
        "  \"\"\" Diacritics in Arabic only aid pronunciation, and serve no semantic or syntactic function. \"\"\"\n",
        "  tweet = ar.strip_tashkeel(tweet)\n",
        "  tweet = ar.strip_tatweel(tweet)\n",
        "\n",
        "  tweet = tweet.replace(\"آ\", \"ا\")\n",
        "  tweet = tweet.replace(\"إ\", \"ا\")\n",
        "  tweet = tweet.replace(\"أ\", \"ا\")\n",
        "  tweet = tweet.replace(\"ؤ\", \"و\")\n",
        "  tweet = tweet.replace(\"ئ\", \"ي\")\n",
        "\n",
        "  return tweet\n",
        "\n",
        "\n",
        "train['tweet'] = train['tweet'].apply(normalise_encoding)\n",
        "train['tweet'] = train['tweet'].apply(remove_lf)\n",
        "train['tweet'] = train['tweet'].apply(remove_diacritics)\n",
        "train['tweet'] = train['tweet'].apply(remove_repeat_chars)\n",
        "\n",
        "# Convert to binary values\n",
        "train['subtask_a'] = train['subtask_a'].apply(lambda x: 1 if x=='NOT' else 0)\n",
        "\n",
        "# Make one df with emoji and one without to compare performance later\n",
        "train_no_emoji = train\n",
        "train_no_emoji['tweet'] = train_no_emoji['tweet'].apply(remove_emoji)\n",
        "\n",
        "dfs = [train, train_no_emoji]"
      ],
      "metadata": {
        "id": "xz6g-O4wbCvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6fb5d66-38be-49a3-cf94-afb66e76edc8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: 'emoji.get_emoji_regexp()' is deprecated and will be removed in version 2.0.0. If you want to remove emoji from a string, consider the method emoji.replace_emoji(str, replace='').\n",
            "To hide this warning, pin/downgrade the package to 'emoji~=1.6.3'\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(10)"
      ],
      "metadata": {
        "id": "ra89z0xLdASD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "b727f2fd-9c35-4918-d5b5-f585377d7505"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                              tweet  subtask_a\n",
              "0   1  الحمدلله يارب فوز مهم يا زمالك.. كل الدعم ليكم...          1\n",
              "1   2            فدوه يا بخت فدوه يا زمن واحد منكم يجيبه          1\n",
              "2   3  RT @USER: يا رب يا واحد يا احد بحق يوم الاحد ا...          0\n",
              "3   4  RT @USER: #هوا_الحرية يا وجع قلبي عليكي يا امي...          1\n",
              "4   5          يا بكون بحياتك الاهم يا اما ما بدي اكون            1\n",
              "5   6  @USER اخخ يا قلببي يا هالحلقه     متعه على بكا...          1\n",
              "6   7  والله الزول السوداني اثبت انه سابق بالتحضر عن ...          1\n",
              "7   8  RT @USER: جالس اسمع احمد قاسم يغني: \"احبك من ك...          1\n",
              "8   9                في قلبي يا مغلاك وبعيني يا محلاك ..          1\n",
              "9  10  يا التاج ع الراس يا السادة يا مالك الروح وراعيها           1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e6c392a-29b7-4937-b118-4a090ab4f087\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>subtask_a</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>الحمدلله يارب فوز مهم يا زمالك.. كل الدعم ليكم...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>فدوه يا بخت فدوه يا زمن واحد منكم يجيبه</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>RT @USER: يا رب يا واحد يا احد بحق يوم الاحد ا...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>RT @USER: #هوا_الحرية يا وجع قلبي عليكي يا امي...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>يا بكون بحياتك الاهم يا اما ما بدي اكون</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>@USER اخخ يا قلببي يا هالحلقه     متعه على بكا...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>والله الزول السوداني اثبت انه سابق بالتحضر عن ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>RT @USER: جالس اسمع احمد قاسم يغني: \"احبك من ك...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>في قلبي يا مغلاك وبعيني يا محلاك ..</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>يا التاج ع الراس يا السادة يا مالك الروح وراعيها</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e6c392a-29b7-4937-b118-4a090ab4f087')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2e6c392a-29b7-4937-b118-4a090ab4f087 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2e6c392a-29b7-4937-b118-4a090ab4f087');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_no_emoji.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "uIH73TGYjMFo",
        "outputId": "f3feb16c-0962-49f5-be08-cd61dd63f323"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                              tweet  subtask_a\n",
              "0   1  الحمدلله يارب فوز مهم يا زمالك.. كل الدعم ليكم...          1\n",
              "1   2            فدوه يا بخت فدوه يا زمن واحد منكم يجيبه          1\n",
              "2   3  RT @USER: يا رب يا واحد يا احد بحق يوم الاحد ا...          0\n",
              "3   4  RT @USER: #هوا_الحرية يا وجع قلبي عليكي يا امي...          1\n",
              "4   5          يا بكون بحياتك الاهم يا اما ما بدي اكون            1\n",
              "5   6  @USER اخخ يا قلببي يا هالحلقه     متعه على بكا...          1\n",
              "6   7  والله الزول السوداني اثبت انه سابق بالتحضر عن ...          1\n",
              "7   8  RT @USER: جالس اسمع احمد قاسم يغني: \"احبك من ك...          1\n",
              "8   9                في قلبي يا مغلاك وبعيني يا محلاك ..          1\n",
              "9  10  يا التاج ع الراس يا السادة يا مالك الروح وراعيها           1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f05fceab-a6b1-48be-9266-76265fc560b0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>subtask_a</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>الحمدلله يارب فوز مهم يا زمالك.. كل الدعم ليكم...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>فدوه يا بخت فدوه يا زمن واحد منكم يجيبه</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>RT @USER: يا رب يا واحد يا احد بحق يوم الاحد ا...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>RT @USER: #هوا_الحرية يا وجع قلبي عليكي يا امي...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>يا بكون بحياتك الاهم يا اما ما بدي اكون</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>@USER اخخ يا قلببي يا هالحلقه     متعه على بكا...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>والله الزول السوداني اثبت انه سابق بالتحضر عن ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>RT @USER: جالس اسمع احمد قاسم يغني: \"احبك من ك...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>في قلبي يا مغلاك وبعيني يا محلاك ..</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>يا التاج ع الراس يا السادة يا مالك الروح وراعيها</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f05fceab-a6b1-48be-9266-76265fc560b0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f05fceab-a6b1-48be-9266-76265fc560b0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f05fceab-a6b1-48be-9266-76265fc560b0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sanity check"
      ],
      "metadata": {
        "id": "qPDFE5AdjiO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_chars(tweet):\n",
        "    return len(tweet)\n",
        "\n",
        "\n",
        "for df in dfs:\n",
        "    df['char_count'] = df['tweet'].apply(count_chars)\n",
        "\n",
        "train_no_emoji.sort_values(by='char_count', ascending=[0]).head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "9s43pncCjkH3",
        "outputId": "587ef190-8052-4542-9d2c-69afb9497ab8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                              tweet  subtask_a  \\\n",
              "2922  2946   من المظاهر الموسفه ان الاب والام  يقعدون يتها...          1   \n",
              "6062  6160   جده الحب ،، جده يا منتهى كل الكلام يا سيدة كل...          1   \n",
              "4761  4833   يا مالي عمري رضا يا شبيه الورد , يا معنى الحي...          1   \n",
              "6564  6707  يا من اظهر الجميل.. وستر القبيح.. يا من لا يوا...          1   \n",
              "2753  2760   يا مكعب السكر يا زمردي الاحمر و ياسري الاكبر ...          1   \n",
              "2124  2125   اللهم اني اسالك مسالة البايس الفقير وادعوك دع...          0   \n",
              "1399  1400  يا مرحبا ترحيبة عقاب يا نوت والحي يلحق لو تحده...          1   \n",
              "2434  2441  يا سيادة الرييس يا سيادة الرييس نعلم ان الحرب ...          0   \n",
              "7196  7358  يا عيباه يا حسافاه اليمني بيهان بكل مكان وهذا ...          1   \n",
              "3050  3122  انت وامثالك وحكامك وعلمايك الزنادقه خنازير دمر...          0   \n",
              "\n",
              "      char_count  \n",
              "2922        5570  \n",
              "6062        5228  \n",
              "4761        3053  \n",
              "6564        2190  \n",
              "2753        2058  \n",
              "2124        1067  \n",
              "1399         284  \n",
              "2434         284  \n",
              "7196         283  \n",
              "3050         283  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-812532d4-11c1-4be8-98bd-346cec4ff8d0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>char_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2922</th>\n",
              "      <td>2946</td>\n",
              "      <td>من المظاهر الموسفه ان الاب والام  يقعدون يتها...</td>\n",
              "      <td>1</td>\n",
              "      <td>5570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6062</th>\n",
              "      <td>6160</td>\n",
              "      <td>جده الحب ،، جده يا منتهى كل الكلام يا سيدة كل...</td>\n",
              "      <td>1</td>\n",
              "      <td>5228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4761</th>\n",
              "      <td>4833</td>\n",
              "      <td>يا مالي عمري رضا يا شبيه الورد , يا معنى الحي...</td>\n",
              "      <td>1</td>\n",
              "      <td>3053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6564</th>\n",
              "      <td>6707</td>\n",
              "      <td>يا من اظهر الجميل.. وستر القبيح.. يا من لا يوا...</td>\n",
              "      <td>1</td>\n",
              "      <td>2190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2753</th>\n",
              "      <td>2760</td>\n",
              "      <td>يا مكعب السكر يا زمردي الاحمر و ياسري الاكبر ...</td>\n",
              "      <td>1</td>\n",
              "      <td>2058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2124</th>\n",
              "      <td>2125</td>\n",
              "      <td>اللهم اني اسالك مسالة البايس الفقير وادعوك دع...</td>\n",
              "      <td>0</td>\n",
              "      <td>1067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>1400</td>\n",
              "      <td>يا مرحبا ترحيبة عقاب يا نوت والحي يلحق لو تحده...</td>\n",
              "      <td>1</td>\n",
              "      <td>284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2434</th>\n",
              "      <td>2441</td>\n",
              "      <td>يا سيادة الرييس يا سيادة الرييس نعلم ان الحرب ...</td>\n",
              "      <td>0</td>\n",
              "      <td>284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7196</th>\n",
              "      <td>7358</td>\n",
              "      <td>يا عيباه يا حسافاه اليمني بيهان بكل مكان وهذا ...</td>\n",
              "      <td>1</td>\n",
              "      <td>283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3050</th>\n",
              "      <td>3122</td>\n",
              "      <td>انت وامثالك وحكامك وعلمايك الزنادقه خنازير دمر...</td>\n",
              "      <td>0</td>\n",
              "      <td>283</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-812532d4-11c1-4be8-98bd-346cec4ff8d0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-812532d4-11c1-4be8-98bd-346cec4ff8d0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-812532d4-11c1-4be8-98bd-346cec4ff8d0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Something is weird with these 6 tweets that have 1000+ characters. Those with 281-284 might be explained by my replacing '\\<LF\\>' with a space to keep word boundaries intact, but 1000+ is clearly some kind of data reading error. I'll just remove them."
      ],
      "metadata": {
        "id": "ecDz6UfOjoup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for df in dfs:\n",
        "    df.drop(df[df['char_count'] > 1000].index, inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "train.sort_values(by='char_count', ascending=[0]).head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "tdB6MzdNjlYF",
        "outputId": "836beb54-c7c8-48e4-e78d-f83e93b0d81d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                              tweet  subtask_a  \\\n",
              "1399  1400  يا مرحبا ترحيبة عقاب يا نوت والحي يلحق لو تحده...          1   \n",
              "2433  2441  يا سيادة الرييس يا سيادة الرييس نعلم ان الحرب ...          0   \n",
              "7190  7358  يا عيباه يا حسافاه اليمني بيهان بكل مكان وهذا ...          1   \n",
              "3047  3122  انت وامثالك وحكامك وعلمايك الزنادقه خنازير دمر...          0   \n",
              "3019  3094  الله ياخذك يا جاهل يا تعبان يا مريض يا معتوه و...          0   \n",
              "475    476  الرساله فيها عشم مش طبيعي بس مش مشكلتي. اولا ك...          0   \n",
              "2653  2661  الهلال امانة في يديكم حنا عند الله ثم عندكم قب...          1   \n",
              "6359  6507  بذلة عسكرية وسط الزحام .. حياة مليية بالاشواك ...          1   \n",
              "7734  7902  #قروب_المشتاقون_للجنه #عبدالرحمن_الخشتي اللهم ...          1   \n",
              "6727  6895  يا امعه يا مغفل  تجيب تغريدات مشجعين مراهقين؟ ...          0   \n",
              "\n",
              "      char_count  \n",
              "1399         284  \n",
              "2433         284  \n",
              "7190         283  \n",
              "3047         283  \n",
              "3019         282  \n",
              "475          282  \n",
              "2653         282  \n",
              "6359         282  \n",
              "7734         282  \n",
              "6727         282  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8ed1fc90-467c-4ab8-82fc-33e4b7752a61\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>char_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>1400</td>\n",
              "      <td>يا مرحبا ترحيبة عقاب يا نوت والحي يلحق لو تحده...</td>\n",
              "      <td>1</td>\n",
              "      <td>284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2433</th>\n",
              "      <td>2441</td>\n",
              "      <td>يا سيادة الرييس يا سيادة الرييس نعلم ان الحرب ...</td>\n",
              "      <td>0</td>\n",
              "      <td>284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7190</th>\n",
              "      <td>7358</td>\n",
              "      <td>يا عيباه يا حسافاه اليمني بيهان بكل مكان وهذا ...</td>\n",
              "      <td>1</td>\n",
              "      <td>283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3047</th>\n",
              "      <td>3122</td>\n",
              "      <td>انت وامثالك وحكامك وعلمايك الزنادقه خنازير دمر...</td>\n",
              "      <td>0</td>\n",
              "      <td>283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3019</th>\n",
              "      <td>3094</td>\n",
              "      <td>الله ياخذك يا جاهل يا تعبان يا مريض يا معتوه و...</td>\n",
              "      <td>0</td>\n",
              "      <td>282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>475</th>\n",
              "      <td>476</td>\n",
              "      <td>الرساله فيها عشم مش طبيعي بس مش مشكلتي. اولا ك...</td>\n",
              "      <td>0</td>\n",
              "      <td>282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2653</th>\n",
              "      <td>2661</td>\n",
              "      <td>الهلال امانة في يديكم حنا عند الله ثم عندكم قب...</td>\n",
              "      <td>1</td>\n",
              "      <td>282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6359</th>\n",
              "      <td>6507</td>\n",
              "      <td>بذلة عسكرية وسط الزحام .. حياة مليية بالاشواك ...</td>\n",
              "      <td>1</td>\n",
              "      <td>282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7734</th>\n",
              "      <td>7902</td>\n",
              "      <td>#قروب_المشتاقون_للجنه #عبدالرحمن_الخشتي اللهم ...</td>\n",
              "      <td>1</td>\n",
              "      <td>282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6727</th>\n",
              "      <td>6895</td>\n",
              "      <td>يا امعه يا مغفل  تجيب تغريدات مشجعين مراهقين؟ ...</td>\n",
              "      <td>0</td>\n",
              "      <td>282</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ed1fc90-467c-4ab8-82fc-33e4b7752a61')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8ed1fc90-467c-4ab8-82fc-33e4b7752a61 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8ed1fc90-467c-4ab8-82fc-33e4b7752a61');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Preprocessing"
      ],
      "metadata": {
        "id": "WU0m50vwjuYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 284\n",
        "\n",
        "def preprocessing_for_bert(data, version=\"mini\"):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-mini-arabic\") if version == \"mini\" else AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n",
        "\n",
        "    # For every sentence...\n",
        "    for i, sent in enumerate(data):\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text = sent,\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,             # Max length to truncate/pad\n",
        "            padding='max_length',           # Pad sentence to max length\n",
        "            return_attention_mask=True,     # Return attention mask\n",
        "            truncation = True\n",
        "            )\n",
        "\n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "metadata": {
        "id": "n_LiOAlijrwc"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-test split"
      ],
      "metadata": {
        "id": "tc02E4O9j1rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tt_split(df):\n",
        "    items, labels = df['tweet'], df['subtask_a']\n",
        "    train_items, test_items, train_labels, test_labels = train_test_split(items, labels, test_size=.15, random_state=42)\n",
        "\n",
        "    train_items = train_items.tolist()\n",
        "    train_labels = train_labels.to_numpy()\n",
        "    test_items = test_items.tolist()\n",
        "    test_labels = test_labels.to_numpy()\n",
        "\n",
        "    return train_items, train_labels, test_items, test_labels\n",
        "\n",
        "\n",
        "data_with_emoji = {}\n",
        "data_without_emoji = {}\n",
        "\n",
        "data_with_emoji['train items'], data_with_emoji['train labels'], data_with_emoji['test items'], data_with_emoji['test labels'] \\\n",
        "    = tt_split(train)\n",
        "data_without_emoji['train items'], data_without_emoji['train labels'], data_without_emoji['test items'], data_without_emoji['test labels'] \\\n",
        "    = tt_split(train_no_emoji)"
      ],
      "metadata": {
        "id": "AtTbf53BjvaX"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dicts = [data_with_emoji, data_without_emoji]\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train and test sets and convert label types to torch.Tensor\n",
        "for d in dicts:\n",
        "    d['train input ids'], d['train masks'] = preprocessing_for_bert(d['train items'])\n",
        "    d['test input ids'], d['test masks'] = preprocessing_for_bert(d['test items'])\n",
        "\n",
        "    d['train labels'] = torch.tensor(d['train labels'])\n",
        "    d['test labels'] = torch.tensor(d['test labels'])"
      ],
      "metadata": {
        "id": "sF8OVAHPj3aE"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "2q17uyh3j9aK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 16\n",
        "\n",
        "## Create the DataLoader for our training set\n",
        "# With emoji:\n",
        "train_data_emoji = TensorDataset(data_with_emoji['train input ids'], data_with_emoji['train masks'], data_with_emoji['train labels'])\n",
        "train_sampler_emoji = RandomSampler(train_data_emoji)\n",
        "train_dataloader_emoji = DataLoader(train_data_emoji, sampler=train_sampler_emoji, batch_size=batch_size)\n",
        "# Without emoji:\n",
        "train_data_no_emoji = TensorDataset(data_without_emoji['train input ids'], data_without_emoji['train masks'], data_without_emoji['train labels'])\n",
        "train_sampler_no_emoji = RandomSampler(train_data_no_emoji)\n",
        "train_dataloader_no_emoji = DataLoader(train_data_no_emoji, sampler=train_sampler_no_emoji, batch_size=batch_size)\n",
        "\n",
        "## Create the DataLoader for our test set\n",
        "# With emoji:\n",
        "test_data_emoji = TensorDataset(data_with_emoji['test input ids'], data_with_emoji['test masks'], data_with_emoji['test labels'])\n",
        "test_sampler_emoji = RandomSampler(test_data_emoji)\n",
        "test_dataloader_emoji = DataLoader(test_data_emoji, sampler=test_sampler_emoji, batch_size=batch_size)\n",
        "# Without emoji:\n",
        "test_data_no_emoji = TensorDataset(data_without_emoji['test input ids'], data_without_emoji['test masks'], data_without_emoji['test labels'])\n",
        "test_sampler_no_emoji = RandomSampler(test_data_no_emoji)\n",
        "test_dataloader_no_emoji = DataLoader(test_data_no_emoji, sampler=test_sampler_no_emoji, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "DBaiJMFoj7PL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the BertClassifier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\" BERT Model for classification tasks. \"\"\"\n",
        "\n",
        "    def __init__(self, freeze_bert=False, version=\"mini\"):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in = 256 if version == \"mini\" else 768\n",
        "        H, D_out = 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = AutoModel.from_pretrained(\"asafaya/bert-mini-arabic\") if version == \"mini\" else AutoModel.from_pretrained(\"asafaya/bert-base-arabic\")\n",
        "        # Instantiate a single-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that holds attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "hZwbT3Pqj-r0"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model(train_dataloader, epochs=4, version=\"mini\"):\n",
        "    \"\"\" Initialize the Bert Classifier, the optimizer and the learning rate scheduler. \"\"\"\n",
        "\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False, version=version)\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(params=list(bert_classifier.parameters()),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "\n",
        "    return bert_classifier, optimizer, scheduler\n",
        "\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\" Set seed for reproducibility. \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)"
      ],
      "metadata": {
        "id": "ZS78rh_1kD3v"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_dataloader, test_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\" Train the BertClassifier model. \"\"\"\n",
        "\n",
        "    # Start training loop\n",
        "    print(\"Starting training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Test Loss':^10} | {'Test Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts += 1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation:\n",
        "            # After the completion of each training epoch, measure the model's performance on our test set.\n",
        "            val_loss, val_accuracy = evaluate(model, test_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "\n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\" After the completion of each training epoch, measure the model's performance on our test set. \"\"\"\n",
        "\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the test set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "# With emoji:\n",
        "bert_classifier_emoji, optimizer, scheduler = initialize_model(train_dataloader_emoji)\n",
        "# Without emoji:\n",
        "bert_classifier_no_emoji, optimizer, scheduler = initialize_model(train_dataloader_no_emoji)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvwFp0_HkGQq",
        "outputId": "04a63a3f-2219-461b-9342-d2fae09a9b4f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at asafaya/bert-mini-arabic were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "Some weights of the model checkpoint at asafaya/bert-mini-arabic were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('WITH EMOJI:')\n",
        "train(bert_classifier_emoji, train_dataloader_emoji, test_dataloader_emoji, evaluation=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxH6wK1bkJK3",
        "outputId": "a801ebf1-a431-4421-d850-81b99b9b2d08"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WITH EMOJI:\n",
            "Starting training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | Test Loss  | Test Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.625824   |     -      |     -     |   1.08   \n",
            "   1    |   40    |   0.627266   |     -      |     -     |   0.94   \n",
            "   1    |   60    |   0.633976   |     -      |     -     |   0.91   \n",
            "   1    |   80    |   0.613156   |     -      |     -     |   0.91   \n",
            "   1    |   100   |   0.631513   |     -      |     -     |   0.91   \n",
            "   1    |   120   |   0.631135   |     -      |     -     |   0.91   \n",
            "   1    |   140   |   0.619276   |     -      |     -     |   0.91   \n",
            "   1    |   160   |   0.595323   |     -      |     -     |   0.91   \n",
            "   1    |   180   |   0.617032   |     -      |     -     |   0.91   \n",
            "   1    |   200   |   0.614294   |     -      |     -     |   0.92   \n",
            "   1    |   220   |   0.625656   |     -      |     -     |   0.92   \n",
            "   1    |   240   |   0.614795   |     -      |     -     |   0.91   \n",
            "   1    |   260   |   0.626745   |     -      |     -     |   0.91   \n",
            "   1    |   280   |   0.624705   |     -      |     -     |   0.92   \n",
            "   1    |   300   |   0.625371   |     -      |     -     |   0.92   \n",
            "   1    |   320   |   0.624587   |     -      |     -     |   0.93   \n",
            "   1    |   340   |   0.614878   |     -      |     -     |   0.92   \n",
            "   1    |   360   |   0.625397   |     -      |     -     |   0.93   \n",
            "   1    |   380   |   0.614484   |     -      |     -     |   0.93   \n",
            "   1    |   400   |   0.606646   |     -      |     -     |   0.93   \n",
            "   1    |   416   |   0.631386   |     -      |     -     |   0.71   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.621029   |  0.614567  |   78.41   |   20.48  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | Test Loss  | Test Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.634393   |     -      |     -     |   0.98   \n",
            "   2    |   40    |   0.616099   |     -      |     -     |   0.93   \n",
            "   2    |   60    |   0.630432   |     -      |     -     |   0.94   \n",
            "   2    |   80    |   0.615958   |     -      |     -     |   0.93   \n",
            "   2    |   100   |   0.631393   |     -      |     -     |   0.94   \n",
            "   2    |   120   |   0.595854   |     -      |     -     |   0.94   \n",
            "   2    |   140   |   0.621363   |     -      |     -     |   0.93   \n",
            "   2    |   160   |   0.625902   |     -      |     -     |   0.94   \n",
            "   2    |   180   |   0.618751   |     -      |     -     |   0.93   \n",
            "   2    |   200   |   0.630774   |     -      |     -     |   0.94   \n",
            "   2    |   220   |   0.632554   |     -      |     -     |   0.94   \n",
            "   2    |   240   |   0.628958   |     -      |     -     |   0.94   \n",
            "   2    |   260   |   0.623605   |     -      |     -     |   0.95   \n",
            "   2    |   280   |   0.613184   |     -      |     -     |   0.95   \n",
            "   2    |   300   |   0.627262   |     -      |     -     |   0.95   \n",
            "   2    |   320   |   0.594923   |     -      |     -     |   0.96   \n",
            "   2    |   340   |   0.616199   |     -      |     -     |   0.95   \n",
            "   2    |   360   |   0.640997   |     -      |     -     |   0.95   \n",
            "   2    |   380   |   0.658368   |     -      |     -     |   0.95   \n",
            "   2    |   400   |   0.612172   |     -      |     -     |   0.95   \n",
            "   2    |   416   |   0.613420   |     -      |     -     |   0.73   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.623098   |  0.614266  |   78.52   |   20.90  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | Test Loss  | Test Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   3    |   20    |   0.638126   |     -      |     -     |   1.01   \n",
            "   3    |   40    |   0.612169   |     -      |     -     |   0.95   \n",
            "   3    |   60    |   0.612925   |     -      |     -     |   0.96   \n",
            "   3    |   80    |   0.626112   |     -      |     -     |   0.96   \n",
            "   3    |   100   |   0.634505   |     -      |     -     |   0.96   \n",
            "   3    |   120   |   0.606061   |     -      |     -     |   0.95   \n",
            "   3    |   140   |   0.611376   |     -      |     -     |   0.95   \n",
            "   3    |   160   |   0.615039   |     -      |     -     |   0.95   \n",
            "   3    |   180   |   0.622210   |     -      |     -     |   0.95   \n",
            "   3    |   200   |   0.640751   |     -      |     -     |   0.95   \n",
            "   3    |   220   |   0.648674   |     -      |     -     |   0.95   \n",
            "   3    |   240   |   0.618191   |     -      |     -     |   0.95   \n",
            "   3    |   260   |   0.650093   |     -      |     -     |   0.95   \n",
            "   3    |   280   |   0.623785   |     -      |     -     |   0.95   \n",
            "   3    |   300   |   0.610788   |     -      |     -     |   0.95   \n",
            "   3    |   320   |   0.620311   |     -      |     -     |   0.94   \n",
            "   3    |   340   |   0.614902   |     -      |     -     |   0.94   \n",
            "   3    |   360   |   0.605350   |     -      |     -     |   0.94   \n",
            "   3    |   380   |   0.605637   |     -      |     -     |   0.94   \n",
            "   3    |   400   |   0.591949   |     -      |     -     |   0.94   \n",
            "   3    |   416   |   0.630591   |     -      |     -     |   0.72   \n",
            "----------------------------------------------------------------------\n",
            "   3    |    -    |   0.620879   |  0.614659  |   78.41   |   20.99  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | Test Loss  | Test Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   4    |   20    |   0.602851   |     -      |     -     |   0.99   \n",
            "   4    |   40    |   0.618665   |     -      |     -     |   0.94   \n",
            "   4    |   60    |   0.625411   |     -      |     -     |   0.94   \n",
            "   4    |   80    |   0.637493   |     -      |     -     |   0.94   \n",
            "   4    |   100   |   0.611342   |     -      |     -     |   0.94   \n",
            "   4    |   120   |   0.622150   |     -      |     -     |   0.94   \n",
            "   4    |   140   |   0.612917   |     -      |     -     |   0.94   \n",
            "   4    |   160   |   0.623740   |     -      |     -     |   0.94   \n",
            "   4    |   180   |   0.619478   |     -      |     -     |   0.94   \n",
            "   4    |   200   |   0.611131   |     -      |     -     |   0.94   \n",
            "   4    |   220   |   0.631118   |     -      |     -     |   0.93   \n",
            "   4    |   240   |   0.607906   |     -      |     -     |   0.93   \n",
            "   4    |   260   |   0.607975   |     -      |     -     |   0.94   \n",
            "   4    |   280   |   0.633916   |     -      |     -     |   1.10   \n",
            "   4    |   300   |   0.615875   |     -      |     -     |   1.19   \n",
            "   4    |   320   |   0.608022   |     -      |     -     |   1.18   \n",
            "   4    |   340   |   0.646062   |     -      |     -     |   0.93   \n",
            "   4    |   360   |   0.614731   |     -      |     -     |   0.93   \n",
            "   4    |   380   |   0.637036   |     -      |     -     |   0.93   \n",
            "   4    |   400   |   0.637987   |     -      |     -     |   0.92   \n",
            "   4    |   416   |   0.637716   |     -      |     -     |   0.71   \n",
            "----------------------------------------------------------------------\n",
            "   4    |    -    |   0.621876   |  0.614308  |   78.63   |   21.35  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('WITHOUT EMOJI:')\n",
        "train(bert_classifier_no_emoji, train_dataloader_no_emoji, test_dataloader_no_emoji, evaluation=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0jLdQX7kaYC",
        "outputId": "057e6f8b-6093-4849-a1ac-7c4d99b436e1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WITHOUT EMOJI:\n",
            "Starting training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | Test Loss  | Test Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.630423   |     -      |     -     |   1.06   \n",
            "   1    |   40    |   0.651899   |     -      |     -     |   1.01   \n",
            "   1    |   60    |   0.647774   |     -      |     -     |   1.01   \n",
            "   1    |   80    |   0.663543   |     -      |     -     |   1.01   \n",
            "   1    |   100   |   0.626367   |     -      |     -     |   1.00   \n",
            "   1    |   120   |   0.627054   |     -      |     -     |   1.00   \n",
            "   1    |   140   |   0.651704   |     -      |     -     |   1.00   \n",
            "   1    |   160   |   0.633709   |     -      |     -     |   1.00   \n",
            "   1    |   180   |   0.645893   |     -      |     -     |   1.00   \n",
            "   1    |   200   |   0.648964   |     -      |     -     |   1.00   \n",
            "   1    |   220   |   0.642811   |     -      |     -     |   1.01   \n",
            "   1    |   240   |   0.650643   |     -      |     -     |   1.01   \n",
            "   1    |   260   |   0.649144   |     -      |     -     |   1.01   \n",
            "   1    |   280   |   0.648201   |     -      |     -     |   1.01   \n",
            "   1    |   300   |   0.657496   |     -      |     -     |   1.00   \n",
            "   1    |   320   |   0.660639   |     -      |     -     |   1.01   \n",
            "   1    |   340   |   0.654384   |     -      |     -     |   1.01   \n",
            "   1    |   360   |   0.655720   |     -      |     -     |   1.01   \n",
            "   1    |   380   |   0.634383   |     -      |     -     |   1.01   \n",
            "   1    |   400   |   0.651158   |     -      |     -     |   1.01   \n",
            "   1    |   416   |   0.656238   |     -      |     -     |   0.78   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.646927   |  0.641164  |   72.06   |   22.23  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | Test Loss  | Test Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.654355   |     -      |     -     |   1.06   \n",
            "   2    |   40    |   0.645107   |     -      |     -     |   1.00   \n",
            "   2    |   60    |   0.658813   |     -      |     -     |   1.02   \n",
            "   2    |   80    |   0.658920   |     -      |     -     |   1.01   \n",
            "   2    |   100   |   0.652897   |     -      |     -     |   1.01   \n",
            "   2    |   120   |   0.641271   |     -      |     -     |   1.01   \n",
            "   2    |   140   |   0.658193   |     -      |     -     |   1.01   \n",
            "   2    |   160   |   0.646783   |     -      |     -     |   1.01   \n",
            "   2    |   180   |   0.646901   |     -      |     -     |   1.02   \n",
            "   2    |   200   |   0.648343   |     -      |     -     |   1.02   \n",
            "   2    |   220   |   0.635817   |     -      |     -     |   1.01   \n",
            "   2    |   240   |   0.659152   |     -      |     -     |   1.00   \n",
            "   2    |   260   |   0.646958   |     -      |     -     |   1.01   \n",
            "   2    |   280   |   0.643061   |     -      |     -     |   1.02   \n",
            "   2    |   300   |   0.637316   |     -      |     -     |   1.01   \n",
            "   2    |   320   |   0.668122   |     -      |     -     |   1.01   \n",
            "   2    |   340   |   0.640382   |     -      |     -     |   1.01   \n",
            "   2    |   360   |   0.645406   |     -      |     -     |   1.02   \n",
            "   2    |   380   |   0.636769   |     -      |     -     |   1.02   \n",
            "   2    |   400   |   0.643857   |     -      |     -     |   1.02   \n",
            "   2    |   416   |   0.658901   |     -      |     -     |   0.79   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.648838   |  0.641683  |   71.84   |   22.35  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | Test Loss  | Test Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   3    |   20    |   0.647489   |     -      |     -     |   1.07   \n",
            "   3    |   40    |   0.638414   |     -      |     -     |   1.01   \n",
            "   3    |   60    |   0.646360   |     -      |     -     |   1.01   \n",
            "   3    |   80    |   0.641259   |     -      |     -     |   1.01   \n",
            "   3    |   100   |   0.642480   |     -      |     -     |   1.02   \n",
            "   3    |   120   |   0.654378   |     -      |     -     |   1.01   \n",
            "   3    |   140   |   0.639130   |     -      |     -     |   1.01   \n",
            "   3    |   160   |   0.660311   |     -      |     -     |   1.02   \n",
            "   3    |   180   |   0.642339   |     -      |     -     |   1.01   \n",
            "   3    |   200   |   0.663145   |     -      |     -     |   1.01   \n",
            "   3    |   220   |   0.651364   |     -      |     -     |   1.01   \n",
            "   3    |   240   |   0.657094   |     -      |     -     |   1.01   \n",
            "   3    |   260   |   0.650311   |     -      |     -     |   1.02   \n",
            "   3    |   280   |   0.669325   |     -      |     -     |   1.01   \n",
            "   3    |   300   |   0.650691   |     -      |     -     |   1.01   \n",
            "   3    |   320   |   0.638109   |     -      |     -     |   1.02   \n",
            "   3    |   340   |   0.638046   |     -      |     -     |   1.02   \n",
            "   3    |   360   |   0.646339   |     -      |     -     |   1.01   \n",
            "   3    |   380   |   0.677747   |     -      |     -     |   1.01   \n",
            "   3    |   400   |   0.666242   |     -      |     -     |   1.01   \n",
            "   3    |   416   |   0.632625   |     -      |     -     |   0.78   \n",
            "----------------------------------------------------------------------\n",
            "   3    |    -    |   0.650314   |  0.640730  |   72.27   |   22.36  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | Test Loss  | Test Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   4    |   20    |   0.664211   |     -      |     -     |   1.07   \n",
            "   4    |   40    |   0.636135   |     -      |     -     |   1.01   \n",
            "   4    |   60    |   0.633968   |     -      |     -     |   1.02   \n",
            "   4    |   80    |   0.646804   |     -      |     -     |   1.01   \n",
            "   4    |   100   |   0.662393   |     -      |     -     |   1.02   \n",
            "   4    |   120   |   0.651921   |     -      |     -     |   1.01   \n",
            "   4    |   140   |   0.659734   |     -      |     -     |   1.01   \n",
            "   4    |   160   |   0.650590   |     -      |     -     |   1.01   \n",
            "   4    |   180   |   0.642108   |     -      |     -     |   1.01   \n",
            "   4    |   200   |   0.651177   |     -      |     -     |   1.01   \n",
            "   4    |   220   |   0.634733   |     -      |     -     |   1.02   \n",
            "   4    |   240   |   0.633764   |     -      |     -     |   1.02   \n",
            "   4    |   260   |   0.637698   |     -      |     -     |   1.01   \n",
            "   4    |   280   |   0.661276   |     -      |     -     |   1.01   \n",
            "   4    |   300   |   0.651384   |     -      |     -     |   1.01   \n",
            "   4    |   320   |   0.643761   |     -      |     -     |   1.01   \n",
            "   4    |   340   |   0.657896   |     -      |     -     |   1.01   \n",
            "   4    |   360   |   0.653027   |     -      |     -     |   1.02   \n",
            "   4    |   380   |   0.639525   |     -      |     -     |   1.01   \n",
            "   4    |   400   |   0.643010   |     -      |     -     |   1.01   \n",
            "   4    |   416   |   0.655784   |     -      |     -     |   0.78   \n",
            "----------------------------------------------------------------------\n",
            "   4    |    -    |   0.648103   |  0.641127  |   72.06   |   22.33  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model for future runs\n",
        "pickle.dump(bert_classifier_emoji, open('trained_model_mini_with_emoji.sav', 'wb'))\n",
        "pickle.dump(bert_classifier_no_emoji, open('trained_model_mini_without_emoji.sav', 'wb'))"
      ],
      "metadata": {
        "id": "evfszdHKk0fA"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "__j5Ke_iRBzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\" Perform a forward pass on the trained BERT model to predict probabilities on the test set. \"\"\"\n",
        "\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "\n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs"
      ],
      "metadata": {
        "id": "AaEoTqBHRCds"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating"
      ],
      "metadata": {
        "id": "PgnJ7rrKRKI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_prec_acc(probs, y_true):\n",
        "    preds = probs[:, 1]\n",
        "\n",
        "    # Get accuracy over the test set\n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "\n",
        "    print(f'Accuracy: {accuracy_score(y_true, y_pred) * 100:.2f}%')\n",
        "    print(f'Precision: {precision_score(y_true, y_pred) * 100:.2f}%')\n",
        "    print(f'Recall: {recall_score(y_true, y_pred) * 100:.2f}%')\n",
        "    print(f'f1-score: {f1_score(y_true, y_pred) * 100:.2f}%')\n",
        "\n",
        "\n",
        "# Compute predicted probabilities on the test set\n",
        "probs_emoji = bert_predict(bert_classifier_emoji, test_dataloader_emoji)\n",
        "probs_no_emoji = bert_predict(bert_classifier_no_emoji, test_dataloader_no_emoji)"
      ],
      "metadata": {
        "id": "JMS4QvQ6REmr"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('--WITH EMOJI--\\n')\n",
        "calculate_prec_acc(probs_emoji, data_with_emoji['test labels'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6OcFmuNRMo9",
        "outputId": "9f8c5bdc-0185-4393-81ed-94c617d34a9c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--WITH EMOJI--\n",
            "\n",
            "Accuracy: 78.98%\n",
            "Precision: 80.30%\n",
            "Recall: 97.77%\n",
            "f1-score: 88.18%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('--WITHOUT EMOJI--\\n')\n",
        "calculate_prec_acc(probs_no_emoji, data_without_emoji['test labels'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwKkdtCIRQD-",
        "outputId": "d528ff6d-67d7-4cc6-e32c-3a6c49f95f5c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--WITHOUT EMOJI--\n",
            "\n",
            "Accuracy: 68.60%\n",
            "Precision: 79.81%\n",
            "Recall: 81.42%\n",
            "f1-score: 80.61%\n"
          ]
        }
      ]
    }
  ]
}